<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.149.1">Hugo</generator>
    <title>Homeassistant on Natsuki&#39;s Tech Blog</title>
            <link href="http://localhost:1313/tags/homeassistant/" rel="alternate" type="text/html" title="html" />
            <link href="http://localhost:1313/tags/homeassistant/index.xml" rel="alternate" type="application/rss+xml" title="rss" />
    <updated>2025-09-09T13:57:30+09:00</updated>
    <id>http://localhost:1313/tags/homeassistant/</id>
        <entry>
            <title>Local Voice Assistant – Docker &#43; Ollama &#43; Intel Arc</title>
            <link href="http://localhost:1313/posts/voicellm/" rel="alternate" type="text/html"  hreflang="en" />
            <id>http://localhost:1313/posts/voicellm/</id>
            <published>2025-09-09T12:00:00+09:00</published>
            <updated>2025-09-09T12:00:00+09:00</updated>
            <content type="html">
                &lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h1 id=&#34;building-my-local-voice-assistant-with-raspberry-pi--local-llms&#34;
    &gt;
        Building My Local Voice Assistant with Raspberry Pi &amp;amp; Local LLMs
    &lt;/h1&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#building-my-local-voice-assistant-with-raspberry-pi--local-llms&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Building My Local Voice Assistant with Raspberry Pi &amp;amp; Local LLMs&#34; href=&#34;#building-my-local-voice-assistant-with-raspberry-pi--local-llms&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;motivation&#34;
    &gt;
        Motivation
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#motivation&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Motivation&#34; href=&#34;#motivation&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;If you want Google Assistant voice control in Home Assistant, you usually need a &lt;strong&gt;Nabu Casa subscription&lt;/strong&gt;. But I wanted two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Save money&lt;/strong&gt; — no monthly subscription.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keep my privacy&lt;/strong&gt; — no smart-home data leaving my LAN.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So I built my own &lt;strong&gt;fully local voice assistant&lt;/strong&gt; — offline, GPU-accelerated, and integrated with Home Assistant.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;inspiration&#34;
    &gt;
        Inspiration
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#inspiration&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Inspiration&#34; href=&#34;#inspiration&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;This idea was inspired by &lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://www.youtube.com/watch?v=XvbVePuP7NY&#34;
&gt;NetworkChuck’s video&lt;/a&gt;. He showed how to run a Pi satellite with systemd. I extended the idea into a cross-platform setup powered by Docker and Ollama.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;my-core-idea&#34;
    &gt;
        My Core Idea
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#my-core-idea&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor My Core Idea&#34; href=&#34;#my-core-idea&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;I wanted everything to &lt;strong&gt;start automatically&lt;/strong&gt;, run &lt;strong&gt;fast on my Intel Arc GPU&lt;/strong&gt;, and stay simple. My setup is built on five pillars:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Docker Desktop + Docker Compose&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whisper (STT) + Piper (TTS) defined in Compose.&lt;/li&gt;
&lt;li&gt;Docker Desktop auto-starts on login → containers always online.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task Scheduler (Windows 11)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Launches Ollama Portable Zip at login.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IPEX-LLM GPU Acceleration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lets Ollama run on my &lt;strong&gt;Intel Arc A580&lt;/strong&gt; instead of CPU.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Qwen 3-4B Instruct&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small but capable LLM for intent recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Instruction Prompt&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuned Qwen so it only returns actionable device control commands (no fluff).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This combo — &lt;strong&gt;Docker Desktop + Compose + Task Scheduler + IPEX Ollama + Qwen 3-4B&lt;/strong&gt; — gives me a fully automated, private assistant.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;hardware-setup&#34;
    &gt;
        Hardware Setup
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#hardware-setup&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Hardware Setup&#34; href=&#34;#hardware-setup&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Raspberry Pi Zero 2 W&lt;/strong&gt; with &lt;strong&gt;ReSpeaker 2-Mic HAT v2&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2.5 W mini speaker&lt;/strong&gt; for audio feedback&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows 11 gaming PC&lt;/strong&gt; with &lt;strong&gt;Intel Arc A580 GPU&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt;, self-hosted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Pi works as a &lt;strong&gt;Wyoming Satellite&lt;/strong&gt;. The Windows 11 PC runs Whisper, Piper, and the LLM backend.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;whisper--piper-docker-compose-on-wsl2&#34;
    &gt;
        Whisper + Piper (Docker Compose on WSL2)
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#whisper--piper-docker-compose-on-wsl2&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Whisper &amp;#43; Piper (Docker Compose on WSL2)&#34; href=&#34;#whisper--piper-docker-compose-on-wsl2&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;docker-composeyml&#34;
    &gt;
        docker-compose.yml
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#docker-composeyml&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor docker-compose.yml&#34; href=&#34;#docker-composeyml&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;services&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;wyoming-whisper&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rhasspy/wyoming-whisper&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;container_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wyoming-whisper&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10300:10300&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;~/whisperdata:/data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--model&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;small-int8&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--language&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;en&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--threads&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;restart&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;unless-stopped&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;wyoming-piper&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rhasspy/wyoming-piper&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;container_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wyoming-piper&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10200:10200&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# If container exposes 5000, use 10200:5000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;~/piperdata:/data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--voice&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;en_US-lessac-medium&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;restart&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;unless-stopped&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With Docker Desktop set to &lt;strong&gt;launch at login&lt;/strong&gt;, these services come online automatically.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;firewall-rules-powershell&#34;
    &gt;
        Firewall Rules (PowerShell)
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#firewall-rules-powershell&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Firewall Rules (PowerShell)&#34; href=&#34;#firewall-rules-powershell&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;New-NetFirewallRule -DisplayName &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Wyoming Piper 10200&amp;#34;&lt;/span&gt;   -Direction Inbound -Protocol TCP -LocalPort &lt;span style=&#34;color:#ae81ff&#34;&gt;10200&lt;/span&gt; -Action Allow
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;New-NetFirewallRule -DisplayName &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Wyoming Whisper 10300&amp;#34;&lt;/span&gt; -Direction Inbound -Protocol TCP -LocalPort &lt;span style=&#34;color:#ae81ff&#34;&gt;10300&lt;/span&gt; -Action Allow
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;New-NetFirewallRule -DisplayName &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ollama 11436&amp;#34;&lt;/span&gt;          -Direction Inbound -Protocol TCP -LocalPort &lt;span style=&#34;color:#ae81ff&#34;&gt;11436&lt;/span&gt; -Action Allow
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;ollama-portable-zip-with-ipex-llm-intel-arc-gpus&#34;
    &gt;
        Ollama Portable Zip with IPEX-LLM (Intel Arc GPUs)
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#ollama-portable-zip-with-ipex-llm-intel-arc-gpus&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Ollama Portable Zip with IPEX-LLM (Intel Arc GPUs)&#34; href=&#34;#ollama-portable-zip-with-ipex-llm-intel-arc-gpus&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;On my Intel Arc A580, I needed GPU acceleration. Since the official Ollama desktop app doesn’t support Intel GPUs, I used the &lt;strong&gt;Portable Zip build + IPEX-LLM&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;batch-script-run-ollama-gpubat&#34;
    &gt;
        Batch script (run-ollama-gpu.bat)
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#batch-script-run-ollama-gpubat&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Batch script (run-ollama-gpu.bat)&#34; href=&#34;#batch-script-run-ollama-gpubat&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@&lt;span style=&#34;color:#66d9ef&#34;&gt;echo&lt;/span&gt; off
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;setlocal&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;set&lt;/span&gt; OLLAMA_ROOT=C:\Ollama-IPEX
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;set&lt;/span&gt; OLLAMA_HOST=0.0.0.0:11436
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;set&lt;/span&gt; OLLAMA_NUM_GPU=999
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;set&lt;/span&gt; OLLAMA_INTEL_GPU=1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;set&lt;/span&gt; SYCL_DEVICE_FILTER=level_zero:gpu
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cd&lt;/span&gt; /d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;%OLLAMA_ROOT%&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;timeout /t 10 /nobreak &amp;gt;nul
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;start&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; /min cmd.exe /c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ollama.exe serve &amp;gt;&amp;gt; &amp;#34;&lt;/span&gt;%OLLAMA_ROOT%\ollama.log&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; 2&amp;gt;&amp;amp;1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;endlocal&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;task-scheduler&#34;
    &gt;
        Task Scheduler
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#task-scheduler&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Task Scheduler&#34; href=&#34;#task-scheduler&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Trigger: &lt;strong&gt;At logon&lt;/strong&gt; (with ~30s delay).&lt;/li&gt;
&lt;li&gt;Action: runs &lt;code&gt;run-ollama-gpu.bat&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;General tab: &lt;strong&gt;Run with highest privileges&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This ensures &lt;strong&gt;Ollama Portable Zip&lt;/strong&gt; is always running in the background with GPU acceleration.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;-note-for-nvidia--apple-users&#34;
    &gt;
        💡 Note for NVIDIA &amp;amp; Apple Users
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#-note-for-nvidia--apple-users&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor 💡 Note for NVIDIA &amp;amp; Apple Users&#34; href=&#34;#-note-for-nvidia--apple-users&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;If you’re on &lt;strong&gt;NVIDIA GPU&lt;/strong&gt; or &lt;strong&gt;Apple Silicon&lt;/strong&gt;, you don’t need this setup. Just install the official &lt;strong&gt;Ollama Desktop app&lt;/strong&gt;, which supports GPU acceleration and auto-starts automatically.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;instruction-prompt-for-qwen&#34;
    &gt;
        Instruction Prompt for Qwen
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#instruction-prompt-for-qwen&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Instruction Prompt for Qwen&#34; href=&#34;#instruction-prompt-for-qwen&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;Background Information&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You are a smart voice assistant for Home Assistant.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You know everything about my home devices and its states.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Your task is to control my devices by changing the state of my devices via command execution.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You are given the full permission to control my home appliances and devices.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;Instructions&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A user will provide a command to turn a device on or off.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You will control the home appliances like lights based on the user&amp;#39;s input.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Below is the example conversation.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;Restrictions&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Don&amp;#39;t give me the structured summary of current states.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Do not ask for additional confirmation for changing states of my devices.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Always respond in plain text only, no controlling characters, no emoji.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NEVER use emoji or any non-alphanumeric characters.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Keep the answer simple and to the point.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;the-result&#34;
    &gt;
        The Result
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#the-result&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor The Result&#34; href=&#34;#the-result&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;The outcome was &lt;strong&gt;surprisingly good&lt;/strong&gt;. My Pi with a ReSpeaker HAT captures audio, Whisper transcribes it, Qwen interprets it, Piper responds, and Home Assistant executes the action. All &lt;strong&gt;fully offline&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;summary&#34;
    &gt;
        Summary
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#summary&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Summary&#34; href=&#34;#summary&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Docker Desktop + Compose&lt;/strong&gt; → perfect for auto-running Whisper and Piper.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task Scheduler + Ollama Portable Zip&lt;/strong&gt; → required for Intel Arc GPUs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IPEX-LLM on Arc A580&lt;/strong&gt; → makes Qwen 3-4B fast enough for real-time use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom prompt&lt;/strong&gt; → essential to keep responses clean and actionable.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;whats-next&#34;
    &gt;
        What’s Next
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;http://localhost:1313/posts/voicellm/#whats-next&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor What’s Next&#34; href=&#34;#whats-next&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Run Whisper and Piper with &lt;strong&gt;GPU acceleration&lt;/strong&gt; on the Intel Arc.&lt;/li&gt;
&lt;li&gt;Try larger models with IPEX-LLM.&lt;/li&gt;
&lt;li&gt;Improve intent handling for complex automations.&lt;/li&gt;
&lt;li&gt;Add &lt;strong&gt;Japanese conversation&lt;/strong&gt; support.&lt;/li&gt;
&lt;/ul&gt;

            </content>   
                                <category scheme="http://localhost:1313/tags/homeassistant" term="homeassistant" label="homeassistant" /> 
                                <category scheme="http://localhost:1313/tags/docker" term="docker" label="docker" /> 
                                <category scheme="http://localhost:1313/tags/ollama" term="ollama" label="ollama" /> 
                                <category scheme="http://localhost:1313/tags/intel" term="intel" label="intel" /> 
                                <category scheme="http://localhost:1313/tags/localai" term="localai" label="localai" /> 
                                <category scheme="http://localhost:1313/tags/privacy" term="privacy" label="privacy" /> 
                                <category scheme="http://localhost:1313/tags/selfhosting" term="selfhosting" label="selfhosting" />
        </entry>
</feed>
